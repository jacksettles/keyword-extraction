{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02a88c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\johnt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\johnt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from PreprocessText import read_doc, preprocess_text\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
    "import torch\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import nltk\n",
    "from nomic import embed\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "333adb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "vectorizer = KeyphraseCountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7630704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_useless_info(patent):\n",
    "    positions_to_remove = {2, 3, 4}\n",
    "    patent = [element for index, element in enumerate(patent) if index not in positions_to_remove]\n",
    "    return patent\n",
    "\n",
    "def remove_allcaps(patent):\n",
    "    patent = [element for index, element in enumerate(patent) if not element.replace(\" \", \"\").isupper()]\n",
    "    return patent\n",
    "\n",
    "def remove_starting_brackets(patent):\n",
    "    for index, element in enumerate(patent):\n",
    "        if element.startswith('[') and re.match(r'\\[\\d+\\]', element):\n",
    "            # Use regular expression to remove brackets and digits\n",
    "            processed_string = re.sub(r'\\[\\d+\\]', '', element)\n",
    "            patent[index] = processed_string\n",
    "    return patent\n",
    "\n",
    "def process_paragraphs(patent: List[str]):\n",
    "    patent = remove_useless_info(patent)\n",
    "    patent = remove_allcaps(patent)\n",
    "    patent = remove_starting_brackets(patent)\n",
    "    return patent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "553db55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\johnt\\Deep_Learning_Assignments\\KeywordExtraction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d79ef49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each patent is a big string, and all of the paragraphs are just separated by '\\n'\n",
    "patents, labels = read_doc(r'C:\\Users\\johnt\\Downloads\\20240106021625210-AMD (2).docx')\n",
    "\n",
    "# Split each document into its paragraphs by splitting on '\\n',\n",
    "# So now each doc is a list of strings\n",
    "# Then process all of these paragraphs according to the process_paragraphs function above\n",
    "# Pop the last element from the list because it is an empty string paragraph\n",
    "for i in range(len(patents)):\n",
    "    patents[i] = patents[i].split('\\n')\n",
    "    patents[i] = process_paragraphs(patents[i])\n",
    "    patents[i].pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d05774ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data.pkl', 'wb') as picklefile:\n",
    "#     pickle.dump((patents, labels), picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6a561ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.pkl', 'rb') as picklefile:\n",
    "    loaded_data = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "73a46456",
   "metadata": {},
   "outputs": [],
   "source": [
    "patents = loaded_data[0]\n",
    "labels = loaded_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56c43c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\"\"\"Ludwig Wittgenstein (1889–1951) was an Austrian-British philosopher whose work \n",
    "        significantly influenced the philosophy of language and mind in the early-to-mid 20th century. \n",
    "        He is renowned for his two major philosophical works, \"Tractatus-Logico-Philosophicus\" and \n",
    "        \"Philosophical Investigations,\" which explore language, meaning, and the nature of thought. \n",
    "        Wittgenstein's later philosophy departed from the earlier logical positivism of the Tractatus, \n",
    "        delving into the complexities of ordinary language and emphasizing the importance of language \n",
    "        games in understanding meaning and communication.\"\"\", \n",
    "             \n",
    "        \"\"\"Roddy White is a retired American football wide receiver who made a significant impact \n",
    "        during his illustrious career in the National Football League (NFL). Born on November 2, 1981,\n",
    "        White played his entire 11-season career with the Atlanta Falcons from 2005 to 2015. Known for \n",
    "        his exceptional route-running, sure hands, and ability to make clutch catches, White became the \n",
    "        Falcons' all-time leading receiver in various statistical categories. He was a four-time Pro Bowler \n",
    "        and was named First Team All-Pro in 2010. White's contributions to the Falcons' offense, coupled \n",
    "        with his leadership on and off the field, solidified his legacy as one of the franchise's all-time \n",
    "        greats. After retiring from professional football, Roddy White remains a beloved figure among Falcons \n",
    "        fans, remembered for his tenacity, skill, and impact on the team's success.\"\"\",\n",
    "       \n",
    "       \"\"\"I am a graduate student at the University of Georgia studying Artificial Intelligence with interests\n",
    "       in machine learning, deep learning, natural language processing, cognitive modeling, as well as the \n",
    "       ethical consequences that surround the field of AI as a whole. I have programming experience in Java \n",
    "       and Python, especially Pandas, Scikit and PyTorch. In deep learning, I have built and worked with \n",
    "       everything from standard feedforward neural networks, to convolutional nets for computer vision tasks, \n",
    "       as well as vision transformers (ViT and DeiT). Before studying Artificial Intelligence, I got my \n",
    "       undergraduate education in Philosophy, Cognitive Science, and Linguistics. Throughout school, I \n",
    "       have also maintained a nationally accredited personal training certification and teach group fitness classes.\n",
    "       I love picking things up (and putting them down), studying consciousness, football, octopi (octopuses? octopodes?),\n",
    "       hip hop, dogs, and philosophy of language (especially the later works of Wittgenstein, and Stanley Cavell).\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b026aa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(doc: List[str], doc_average=None, num_keywords=10):\n",
    "    document_keyphrase_matrix = vectorizer.fit_transform(doc).toarray()\n",
    "    keyphrases = vectorizer.get_feature_names_out()\n",
    "\n",
    "    doc_tokens = [tokenizer(paragraph, return_tensors=\"pt\", add_special_tokens=False, max_length=512, truncation=True) for paragraph in doc]\n",
    "    keyphrase_tokens = [tokenizer(phrase, return_tensors=\"pt\", add_special_tokens=False) for phrase in keyphrases]\n",
    "    \n",
    "    paragraph_embeddings_tensor, paragraph_embeddings_list = embed_paragraphs(doc_tokens)\n",
    "    kp_embeddings_tensor, kp_embeddings_list = embed_keyphrases(keyphrase_tokens)\n",
    "    \n",
    "    if doc_average == \"keyphrase\":\n",
    "        doc_embedding = embed_doc_by_keyphrase(paragraph_embeddings_tensor, kp_embeddings_tensor, kp_embeddings_list)\n",
    "    else:\n",
    "        doc_embedding = embed_doc_by_paragraph(paragraph_embeddings_tensor)\n",
    "    \n",
    "    if num_keywords > len(keyphrases):\n",
    "        num_keywords = len(keyphrases)\n",
    "        \n",
    "    doc_kp_sim = cosine_similarity(doc_embedding, kp_embeddings_tensor)\n",
    "    top_k_kp = np.argsort(doc_kp_sim)[0][::-1][:num_keywords]\n",
    "    \n",
    "    top_keyphrases = [keyphrases[x] for x in top_k_kp]\n",
    "    \n",
    "    return top_keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b1c9b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_doc_by_keyphrase(paragraph_embeddings_tensor, kp_embeddings_tensor, kp_embeddings_list):\n",
    "    cosine_similarities = cosine_similarity(paragraph_embeddings_tensor, kp_embeddings_tensor)\n",
    "    paragraph_kw_indices = np.argmax(cosine_similarities, axis=1)\n",
    "        \n",
    "    paragraph_kw_embeddings = []\n",
    "    for x in paragraph_kw_indices:\n",
    "        paragraph_kw_embeddings.append(kp_embeddings_list[x])\n",
    "        \n",
    "    paragraph_kw_embeddings = torch.stack(paragraph_kw_embeddings, dim=0)\n",
    "    paragraph_kw_embeddings = paragraph_kw_embeddings.squeeze(1)\n",
    "    doc_embedding = paragraph_kw_embeddings.mean(dim=0)\n",
    "    doc_embedding = doc_embedding.unsqueeze(0)\n",
    "    return doc_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae8609ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_doc_by_paragraph(paragraph_embeddings_tensor):\n",
    "    doc_embedding = paragraph_embeddings_tensor.mean(axis=0)\n",
    "    doc_embedding = doc_embedding.unsqueeze(0)\n",
    "    return doc_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d807cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_paragraphs(doc_tokens: List[transformers.tokenization_utils_base.BatchEncoding]):\n",
    "    paragraph_embeddings = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, token in enumerate(doc_tokens):\n",
    "            outputs = model(**token)\n",
    "            embeddings = outputs.last_hidden_state\n",
    "            embeddings = embeddings.mean(dim=1)\n",
    "            paragraph_embeddings.append(embeddings)\n",
    "\n",
    "    paragraph_embeddings_stacked = torch.stack(paragraph_embeddings, dim=0)\n",
    "    paragraph_embeddings_stacked = paragraph_embeddings_stacked.squeeze(1)\n",
    "    \n",
    "    return paragraph_embeddings_stacked, paragraph_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7116d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_keyphrases(keyphrase_tokens: List[transformers.tokenization_utils_base.BatchEncoding]):\n",
    "    kp_embeddings = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for token in keyphrase_tokens:\n",
    "            kp_output = model(**token)\n",
    "            kp_embedding = kp_output.last_hidden_state\n",
    "            kp_embedding = kp_embedding.mean(dim=1)\n",
    "            kp_embeddings.append(kp_embedding)\n",
    "\n",
    "    kp_embeddings_stacked = torch.stack(kp_embeddings, dim=0)\n",
    "    kp_embeddings_stacked = kp_embeddings_stacked.squeeze(1)\n",
    "    \n",
    "    return kp_embeddings_stacked, kp_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd20526d",
   "metadata": {},
   "source": [
    "# Test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1649df6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bloop = get_keywords(patents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea9475c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['å.',\n",
       " 'gate dielectric layer',\n",
       " 'gate dielectric material',\n",
       " 'molybdenum',\n",
       " 'polycrystalline silicon gate',\n",
       " 'metal gate electrode',\n",
       " 'best mode',\n",
       " 'various obvious respects',\n",
       " 'first polycrystalline silicon layer',\n",
       " 'other embodiments']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bloop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7271e7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "label2pat = dict(zip(labels, patents))\n",
    "patent_keyphrase = {}\n",
    "\n",
    "i = 0\n",
    "for label, patent in label2pat.items():\n",
    "    print(i)\n",
    "    i += 1\n",
    "    keyphrases = get_keywords(patent)\n",
    "    \n",
    "    patent_keyphrase[label] = keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f17c4c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_df = pd.DataFrame(patent_keyphrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7755b4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_df = patent_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2b26f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_df.to_csv('.\\patent_to_keyword.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903e6c8c",
   "metadata": {},
   "source": [
    "# Other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74317030",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'mixedbread-ai/mxbai-embed-large-v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aadd872",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1564e8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f31bc0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cos_sim\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# For retrieval you need to pass this prompt. Please find our more in our blog post.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform_query\u001b[39m(query: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "# For retrieval you need to pass this prompt. Please find our more in our blog post.\n",
    "def transform_query(query: str) -> str:\n",
    "    \"\"\" For retrieval, add the prompt for query (not for documents).\n",
    "    \"\"\"\n",
    "    return f'Represent this sentence for searching relevant passages: {query}'\n",
    "\n",
    "# The model works really well with cls pooling (default) but also with mean poolin.\n",
    "def pooling(outputs: torch.Tensor, inputs: Dict,  strategy: str = 'cls') -> np.ndarray:\n",
    "    if strategy == 'cls':\n",
    "        outputs = outputs[:, 0]\n",
    "    elif strategy == 'mean':\n",
    "        outputs = torch.sum(\n",
    "            outputs * inputs[\"attention_mask\"][:, :, None], dim=1) / torch.sum(inputs[\"attention_mask\"])\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return outputs.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b68edda4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transform_query' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m docs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mtransform_query\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA man is eating a piece of bread\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA man is eating food.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA man is eating pasta.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe girl is carrying a baby.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA man is riding a horse.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m ]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 2. encode\u001b[39;00m\n\u001b[0;32m     10\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(docs, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transform_query' is not defined"
     ]
    }
   ],
   "source": [
    "docs = [\n",
    "    transform_query('A man is eating a piece of bread'),\n",
    "    \"A man is eating food.\",\n",
    "    \"A man is eating pasta.\",\n",
    "    \"The girl is carrying a baby.\",\n",
    "    \"A man is riding a horse.\",\n",
    "]\n",
    "\n",
    "# 2. encode\n",
    "inputs = tokenizer(docs, padding=True, return_tensors='pt')\n",
    "for k, v in inputs.items():\n",
    "    inputs[k] = v.cuda()\n",
    "outputs = model(**inputs).last_hidden_state\n",
    "embeddings = pooling(outputs, inputs, 'cls')\n",
    "\n",
    "similarities = cos_sim(embeddings[0], embeddings[1:])\n",
    "print('similarities:', similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add7839f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
