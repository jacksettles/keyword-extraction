{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02a88c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\johnt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\johnt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from PreprocessText import read_doc, preprocess_text\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
    "import torch\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import nltk\n",
    "from nomic import embed\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "768ef23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. Using CPU.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # CUDA is available, you can proceed to use it\n",
    "    device = torch.device('cuda')\n",
    "    print('CUDA is available. Using GPU.')\n",
    "else:\n",
    "    # CUDA is not available, use CPU\n",
    "    device = torch.device('cpu')\n",
    "    print('CUDA is not available. Using CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "333adb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "vectorizer = KeyphraseCountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7630704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_useless_info(patent):\n",
    "    positions_to_remove = {2, 3, 4}\n",
    "    patent = [element for index, element in enumerate(patent) if index not in positions_to_remove]\n",
    "    return patent\n",
    "\n",
    "def remove_allcaps(patent):\n",
    "    patent = [element for index, element in enumerate(patent) if not element.replace(\" \", \"\").isupper()]\n",
    "    return patent\n",
    "\n",
    "def remove_starting_brackets(patent):\n",
    "    for index, element in enumerate(patent):\n",
    "        if element.startswith('[') and re.match(r'\\[\\d+\\]', element):\n",
    "            # Use regular expression to remove brackets and digits\n",
    "            processed_string = re.sub(r'\\[\\d+\\]', '', element)\n",
    "            patent[index] = processed_string\n",
    "    return patent\n",
    "\n",
    "def process_paragraphs(patent: List[str]):\n",
    "    patent = remove_useless_info(patent)\n",
    "    patent = remove_allcaps(patent)\n",
    "    patent = remove_starting_brackets(patent)\n",
    "    return patent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "553db55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = r'C:\\Users\\johnt\\Deep_Learning_Assignments\\KeywordExtraction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d79ef49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each patent is a big string, and all of the paragraphs are just separated by '\\n'\n",
    "# patents, labels = read_doc(r'C:\\Users\\johnt\\Downloads\\20240106021625210-AMD (2).docx')\n",
    "\n",
    "# Split each document into its paragraphs by splitting on '\\n',\n",
    "# So now each doc is a list of strings\n",
    "# Then process all of these paragraphs according to the process_paragraphs function above\n",
    "# Pop the last element from the list because it is an empty string paragraph\n",
    "# for i in range(len(patents)):\n",
    "#     patents[i] = patents[i].split('\\n')\n",
    "#     patents[i] = process_paragraphs(patents[i])\n",
    "#     patents[i].pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d05774ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data.pkl', 'wb') as picklefile:\n",
    "#     pickle.dump((patents, labels), picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6a561ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.pkl', 'rb') as picklefile:\n",
    "    loaded_data = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "73a46456",
   "metadata": {},
   "outputs": [],
   "source": [
    "patents = loaded_data[0]\n",
    "labels = loaded_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b026aa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(doc: List[str], doc_average=None, num_keywords=10):\n",
    "    document_keyphrase_matrix = vectorizer.fit_transform(doc).toarray()\n",
    "    keyphrases = vectorizer.get_feature_names_out()\n",
    "\n",
    "    doc_tokens = [tokenizer(paragraph, return_tensors=\"pt\", add_special_tokens=False, max_length=512, truncation=True) for paragraph in doc]\n",
    "    keyphrase_tokens = [tokenizer(phrase, return_tensors=\"pt\", add_special_tokens=False) for phrase in keyphrases]\n",
    "    \n",
    "    paragraph_embeddings_tensor, paragraph_embeddings_list = embed_paragraphs(doc_tokens)\n",
    "    kp_embeddings_tensor, kp_embeddings_list = embed_keyphrases(keyphrase_tokens)\n",
    "    \n",
    "    if doc_average == \"keyphrase\":\n",
    "        doc_embedding = embed_doc_by_keyphrase(paragraph_embeddings_tensor, kp_embeddings_tensor, kp_embeddings_list)\n",
    "    else:\n",
    "        doc_embedding = embed_doc_by_paragraph(paragraph_embeddings_tensor)\n",
    "    \n",
    "    if num_keywords > len(keyphrases):\n",
    "        num_keywords = len(keyphrases)\n",
    "        \n",
    "    doc_kp_sim = cosine_similarity(doc_embedding, kp_embeddings_tensor)\n",
    "    top_k_kp = np.argsort(doc_kp_sim)[0][::-1][:num_keywords]\n",
    "    \n",
    "    top_keyphrases = [keyphrases[x] for x in top_k_kp]\n",
    "    \n",
    "    return top_keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3b1c9b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_doc_by_keyphrase(paragraph_embeddings_tensor, kp_embeddings_tensor, kp_embeddings_list):\n",
    "    cosine_similarities = cosine_similarity(paragraph_embeddings_tensor, kp_embeddings_tensor)\n",
    "    paragraph_kw_indices = np.argmax(cosine_similarities, axis=1)\n",
    "        \n",
    "    paragraph_kw_embeddings = []\n",
    "    for x in paragraph_kw_indices:\n",
    "        paragraph_kw_embeddings.append(kp_embeddings_list[x])\n",
    "        \n",
    "    paragraph_kw_embeddings = torch.stack(paragraph_kw_embeddings, dim=0)\n",
    "    paragraph_kw_embeddings = paragraph_kw_embeddings.squeeze(1)\n",
    "    doc_embedding = paragraph_kw_embeddings.mean(dim=0)\n",
    "    doc_embedding = doc_embedding.unsqueeze(0)\n",
    "    return doc_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ae8609ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_doc_by_paragraph(paragraph_embeddings_tensor):\n",
    "    doc_embedding = paragraph_embeddings_tensor.mean(axis=0)\n",
    "    doc_embedding = doc_embedding.unsqueeze(0)\n",
    "    return doc_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d807cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_paragraphs(doc_tokens: List[transformers.tokenization_utils_base.BatchEncoding]):\n",
    "    paragraph_embeddings = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, token in enumerate(doc_tokens):\n",
    "            outputs = model(**token)\n",
    "            embeddings = outputs.last_hidden_state\n",
    "            embeddings = embeddings.mean(dim=1)\n",
    "            paragraph_embeddings.append(embeddings)\n",
    "\n",
    "    paragraph_embeddings_stacked = torch.stack(paragraph_embeddings, dim=0)\n",
    "    paragraph_embeddings_stacked = paragraph_embeddings_stacked.squeeze(1)\n",
    "    \n",
    "    return paragraph_embeddings_stacked, paragraph_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c7116d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_keyphrases(keyphrase_tokens: List[transformers.tokenization_utils_base.BatchEncoding]):\n",
    "    kp_embeddings = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for token in keyphrase_tokens:\n",
    "            kp_output = model(**token)\n",
    "            kp_embedding = kp_output.last_hidden_state\n",
    "            kp_embedding = kp_embedding.mean(dim=1)\n",
    "            kp_embeddings.append(kp_embedding)\n",
    "\n",
    "    kp_embeddings_stacked = torch.stack(kp_embeddings, dim=0)\n",
    "    kp_embeddings_stacked = kp_embeddings_stacked.squeeze(1)\n",
    "    \n",
    "    return kp_embeddings_stacked, kp_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd20526d",
   "metadata": {},
   "source": [
    "# Test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1649df6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_keywords(patents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea9475c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ã¥.',\n",
       " 'gate dielectric layer',\n",
       " 'gate dielectric material',\n",
       " 'molybdenum',\n",
       " 'polycrystalline silicon gate',\n",
       " 'metal gate electrode',\n",
       " 'best mode',\n",
       " 'various obvious respects',\n",
       " 'first polycrystalline silicon layer',\n",
       " 'other embodiments']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7271e7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "label2pat = dict(zip(labels, patents))\n",
    "patent_keyphrase = {}\n",
    "\n",
    "i = 0\n",
    "for label, patent in label2pat.items():\n",
    "    print(i)\n",
    "    i += 1\n",
    "    keyphrases = get_keywords(patent)\n",
    "    \n",
    "    patent_keyphrase[label] = keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f17c4c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_df = pd.DataFrame(patent_keyphrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7755b4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_df = patent_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2b26f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If working on Linux, might need to change the '\\' to '/' in the file name\n",
    "patent_df.to_csv('.\\patent_to_keyword.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903e6c8c",
   "metadata": {},
   "source": [
    "# Other models\n",
    "#### This was just some very hacky testing on other potential models, didn't get very far\n",
    "\n",
    "#### Check out this link for KW extraction models: https://huggingface.co/spaces/mteb/leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74317030",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'mixedbread-ai/mxbai-embed-large-v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aadd872",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1564e8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f31bc0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cos_sim\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# For retrieval you need to pass this prompt. Please find our more in our blog post.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform_query\u001b[39m(query: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "# For retrieval you need to pass this prompt. Please find our more in our blog post.\n",
    "def transform_query(query: str) -> str:\n",
    "    \"\"\" For retrieval, add the prompt for query (not for documents).\n",
    "    \"\"\"\n",
    "    return f'Represent this sentence for searching relevant passages: {query}'\n",
    "\n",
    "# The model works really well with cls pooling (default) but also with mean poolin.\n",
    "def pooling(outputs: torch.Tensor, inputs: Dict,  strategy: str = 'cls') -> np.ndarray:\n",
    "    if strategy == 'cls':\n",
    "        outputs = outputs[:, 0]\n",
    "    elif strategy == 'mean':\n",
    "        outputs = torch.sum(\n",
    "            outputs * inputs[\"attention_mask\"][:, :, None], dim=1) / torch.sum(inputs[\"attention_mask\"])\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return outputs.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b68edda4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transform_query' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m docs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mtransform_query\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA man is eating a piece of bread\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA man is eating food.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA man is eating pasta.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe girl is carrying a baby.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA man is riding a horse.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m ]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 2. encode\u001b[39;00m\n\u001b[0;32m     10\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(docs, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transform_query' is not defined"
     ]
    }
   ],
   "source": [
    "docs = [\n",
    "    transform_query('A man is eating a piece of bread'),\n",
    "    \"A man is eating food.\",\n",
    "    \"A man is eating pasta.\",\n",
    "    \"The girl is carrying a baby.\",\n",
    "    \"A man is riding a horse.\",\n",
    "]\n",
    "\n",
    "# 2. encode\n",
    "inputs = tokenizer(docs, padding=True, return_tensors='pt')\n",
    "for k, v in inputs.items():\n",
    "    inputs[k] = v.cuda()\n",
    "outputs = model(**inputs).last_hidden_state\n",
    "embeddings = pooling(outputs, inputs, 'cls')\n",
    "\n",
    "similarities = cos_sim(embeddings[0], embeddings[1:])\n",
    "print('similarities:', similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add7839f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
